#!/bin/bash
set -e

ADMIN_USER="__ADMIN_USER__"
BIELIK_MODEL="__BIELIK_MODEL__"

echo "======================================"
echo "Rozpoczynam instalacjÄ™ Ollama i Bielik"
echo "======================================"
echo "UÅ¼ytkownik: $ADMIN_USER"
echo "Model: $BIELIK_MODEL"

# Aktualizacja systemu
echo "Aktualizacja pakietÃ³w..."
export DEBIAN_FRONTEND=noninteractive
apt-get update
apt-get upgrade -y

# Instalacja zaleÅ¼noÅ›ci
echo "Instalacja zaleÅ¼noÅ›ci..."
apt-get install -y curl wget git htop

# Instalacja Ollama
echo "Instalacja Ollama..."
curl -fsSL https://ollama.com/install.sh | sh

# Czekaj aÅ¼ usÅ‚uga Ollama siÄ™ uruchomi
echo "Oczekiwanie na uruchomienie Ollama..."
sleep 5

# SprawdÅº status usÅ‚ugi
systemctl status ollama --no-pager || true

# Konfiguracja Ollama do nasÅ‚uchiwania na wszystkich interfejsach
echo "Konfiguracja Ollama..."
mkdir -p /etc/systemd/system/ollama.service.d
cat > /etc/systemd/system/ollama.service.d/override.conf << 'EOF'
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
EOF

# Restart usÅ‚ugi Ollama
systemctl daemon-reload
systemctl restart ollama
sleep 5

# Pobieranie modelu Bielik v2.6 z HuggingFace
echo "Pobieranie modelu Bielik v2.6 z HuggingFace (moÅ¼e to potrwaÄ‡ 10-15 minut)..."

# Model jest dostÄ™pny tylko jako GGUF na HuggingFace, nie w bibliotece Ollama
GGUF_URL="https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct-GGUF/resolve/main/Bielik-11B-v2.6-Instruct.Q4_K_M.gguf"
GGUF_FILE="/tmp/bielik-v2.6-q4km.gguf"
MODELFILE="/tmp/Modelfile.bielik-v2.6"

# Pobierz plik GGUF
echo "Pobieranie pliku GGUF (6.72GB)..."
wget -q --show-progress -O "${GGUF_FILE}" "${GGUF_URL}" || {
    echo "âŒ BÅ‚Ä…d pobierania pliku GGUF"
    exit 1
}

# UtwÃ³rz Modelfile zgodnie z dokumentacjÄ… HuggingFace
echo "Tworzenie Modelfile..."
cat > "${MODELFILE}" << 'MODELFILE_EOF'
FROM /tmp/bielik-v2.6-q4km.gguf

TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""

PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
PARAMETER temperature 0.6
PARAMETER top_p 0.9
MODELFILE_EOF

# UtwÃ³rz model w Ollama z pobranego GGUF
echo "Tworzenie modelu w Ollama..."
su - ${ADMIN_USER} -c "ollama create ${BIELIK_MODEL} -f ${MODELFILE}" || {
    echo "âŒ BÅ‚Ä…d tworzenia modelu w Ollama"
    exit 1
}

# Opcjonalne: UsuÅ„ pliki tymczasowe (GGUF jest ~7GB)
# rm -f "${GGUF_FILE}" "${MODELFILE}"
echo "Plik GGUF pozostawiony w: ${GGUF_FILE}"

# Weryfikacja instalacji
echo "Weryfikacja instalacji..."
su - ${ADMIN_USER} -c "ollama list"

# Tworzenie skryptu testowego
cat > /home/${ADMIN_USER}/test-bielik.sh << TESTEOF
#!/bin/bash
echo "Test Bielik API..."
curl http://localhost:11434/api/chat -d '{
  "model": "${BIELIK_MODEL}",
  "stream": false,
  "messages": [
    {
      "role": "user",
      "content": "Kim jest Adam Mickiewicz?"
    }
  ]
}'
TESTEOF

chmod +x /home/${ADMIN_USER}/test-bielik.sh
chown ${ADMIN_USER}:${ADMIN_USER} /home/${ADMIN_USER}/test-bielik.sh

# Tworzenie skryptu informacyjnego
cat > /home/${ADMIN_USER}/bielik-info.txt << INFOEOF
============================================
Bielik + Ollama - Informacje o instalacji
============================================

âœ… Instalacja zakoÅ„czona pomyÅ›lnie!

Model: ${BIELIK_MODEL}

ðŸ“ Podstawowe komendy:

1. SprawdÅº zainstalowane modele:
   ollama list

2. Uruchom model interaktywnie:
   ollama run ${BIELIK_MODEL}

3. Test API:
   ./test-bielik.sh

4. Status usÅ‚ugi Ollama:
   systemctl status ollama

5. Logi Ollama:
   journalctl -u ollama -f

ðŸŒ API Endpoint:
   http://localhost:11434

ðŸ“š Dokumentacja Ollama:
   https://github.com/ollama/ollama

ðŸ‡µðŸ‡± Model Bielik v2.6:
   https://huggingface.co/speakleash/Bielik-11B-v2.6-Instruct-GGUF

âš ï¸ Uwaga:
   Model v2.6 jest instalowany z HuggingFace (GGUF),
   poniewaÅ¼ nie jest jeszcze dostÄ™pny w bibliotece Ollama.
   Plik GGUF (~7GB) znajduje siÄ™ w /tmp/bielik-v2.6-q4km.gguf
INFOEOF

chown ${ADMIN_USER}:${ADMIN_USER} /home/${ADMIN_USER}/bielik-info.txt

echo "======================================"
echo "âœ… Instalacja zakoÅ„czona!"
echo "======================================"
echo ""
echo "Ollama API: http://localhost:11434"
echo "Model: ${BIELIK_MODEL}"
echo ""
echo "SprawdÅº: cat ~/bielik-info.txt"

exit 0
